{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3752844",
   "metadata": {},
   "source": [
    "#### LANGUAGE PARSING\n",
    "\n",
    "<br>\n",
    "\n",
    "# Discover Insights into Classic Texts\n",
    "\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7354cf05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\xemyc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3370f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag, RegexpParser\n",
    "from tokenize_words import word_sentence_tokenize\n",
    "from chunk_counters import np_chunk_counter, vp_chunk_counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89bd695b",
   "metadata": {},
   "source": [
    "### Import and Preprocess Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06cd1f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open(\"dorian_gray.txt\", encoding = 'utf-8').read().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8eba2367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'picture', 'of', 'dorian', 'gray', 'by', 'oscar', 'wilde', 'the', 'preface', 'the', 'artist', 'is', 'the', 'creator', 'of', 'beautiful', 'things', '.']\n"
     ]
    }
   ],
   "source": [
    "# breaks the sentences into arrays of words\n",
    "word_tokenized_text = word_sentence_tokenize(text)\n",
    "print(word_tokenized_text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e71c4e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['it', 'seems', 'to', 'be', 'the', 'one', 'thing', 'that', 'can', 'make', 'modern', 'life', 'mysterious', 'or', 'marvellous', 'to', 'us', '.']\n"
     ]
    }
   ],
   "source": [
    "# store and print 100th sentence word tokenized sentence\n",
    "single_word_tokenized_sentence = word_tokenized_text[100]\n",
    "print(single_word_tokenized_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1170699b",
   "metadata": {},
   "source": [
    "### Part-of-Speech Tag Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5c06fe8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# part-of-speech tag each sentence and append to list of pos-tagged sentences\n",
    "pos_tagged_text = []\n",
    "\n",
    "for word in word_tokenized_text:\n",
    "    pos_tagged_text.append(pos_tag(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a6953851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('it', 'PRP'), ('seems', 'VBZ'), ('to', 'TO'), ('be', 'VB'), ('the', 'DT'), ('one', 'CD'), ('thing', 'NN'), ('that', 'WDT'), ('can', 'MD'), ('make', 'VB'), ('modern', 'JJ'), ('life', 'NN'), ('mysterious', 'JJ'), ('or', 'CC'), ('marvellous', 'JJ'), ('to', 'TO'), ('us', 'PRP'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# store and print 100th sentence part-of-speech tagged sentence\n",
    "single_pos_sentence = pos_tagged_text[100]\n",
    "print(single_pos_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6176818",
   "metadata": {},
   "source": [
    "### Chunk Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9f7a3c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define noun phrase chunk grammar\n",
    "np_chunk_grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "\n",
    "# create noun phrase RegexpParser object\n",
    "np_chunk_parser = RegexpParser(np_chunk_grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "977b9051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define verb phrase chunk grammar\n",
    "vp_chunk_grammar = \"VP: {<DT>?<JJ>*<NN><VB.*><RB.?>?}\"\n",
    "\n",
    "# create verb phrase RegexpParser object\n",
    "vp_chunk_parser = RegexpParser(vp_chunk_grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "187f1bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_chunked_text = []\n",
    "vp_chunked_text = []\n",
    "\n",
    "# chunk each sentence and append to lists\n",
    "for chunk in pos_tagged_text:\n",
    "  np_chunked_text.append(np_chunk_parser.parse(chunk))\n",
    "  vp_chunked_text.append(vp_chunk_parser.parse(chunk))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895ec17d",
   "metadata": {},
   "source": [
    "### Analyze Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1b049054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[((('i', 'NN'),), 963), ((('henry', 'NN'),), 200), ((('lord', 'NN'),), 197), ((('life', 'NN'),), 170), ((('harry', 'NN'),), 136), ((('dorian', 'JJ'), ('gray', 'NN')), 127), ((('something', 'NN'),), 126), ((('nothing', 'NN'),), 93), ((('basil', 'NN'),), 85), ((('the', 'DT'), ('world', 'NN')), 70), ((('everything', 'NN'),), 69), ((('anything', 'NN'),), 68), ((('hallward', 'NN'),), 68), ((('the', 'DT'), ('man', 'NN')), 61), ((('the', 'DT'), ('room', 'NN')), 60), ((('face', 'NN'),), 57), ((('the', 'DT'), ('door', 'NN')), 56), ((('love', 'NN'),), 55), ((('art', 'NN'),), 52), ((('course', 'NN'),), 51), ((('the', 'DT'), ('picture', 'NN')), 46), ((('the', 'DT'), ('lad', 'NN')), 45), ((('head', 'NN'),), 44), ((('round', 'NN'),), 44), ((('hand', 'NN'),), 44), ((('sibyl', 'NN'),), 41), ((('the', 'DT'), ('table', 'NN')), 40), ((('the', 'DT'), ('painter', 'NN')), 38), ((('sir', 'NN'),), 38), ((('a', 'DT'), ('moment', 'NN')), 38)]\n"
     ]
    }
   ],
   "source": [
    "# store and print the most common NP-chunks\n",
    "most_common_np_chunks = np_chunk_counter(np_chunked_text)\n",
    "print(most_common_np_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4647a546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[((('i', 'NN'), ('am', 'VBP')), 101), ((('i', 'NN'), ('was', 'VBD')), 40), ((('i', 'NN'), ('want', 'VBP')), 37), ((('i', 'NN'), ('know', 'VBP')), 33), ((('i', 'NN'), ('do', 'VBP'), (\"n't\", 'RB')), 32), ((('i', 'NN'), ('have', 'VBP')), 32), ((('i', 'NN'), ('had', 'VBD')), 31), ((('i', 'NN'), ('suppose', 'VBP')), 17), ((('i', 'NN'), ('think', 'VBP')), 16), ((('i', 'NN'), ('am', 'VBP'), ('not', 'RB')), 14), ((('i', 'NN'), ('thought', 'VBD')), 13), ((('i', 'NN'), ('believe', 'VBP')), 12), ((('dorian', 'JJ'), ('gray', 'NN'), ('was', 'VBD')), 11), ((('i', 'NN'), ('am', 'VBP'), ('so', 'RB')), 11), ((('henry', 'NN'), ('had', 'VBD')), 11), ((('i', 'NN'), ('did', 'VBD'), (\"n't\", 'RB')), 9), ((('i', 'NN'), ('met', 'VBD')), 9), ((('i', 'NN'), ('said', 'VBD')), 9), ((('i', 'NN'), ('am', 'VBP'), ('quite', 'RB')), 8), ((('i', 'NN'), ('see', 'VBP')), 8), ((('i', 'NN'), ('did', 'VBD'), ('not', 'RB')), 7), ((('i', 'NN'), ('have', 'VBP'), ('ever', 'RB')), 7), ((('life', 'NN'), ('has', 'VBZ')), 7), ((('i', 'NN'), ('did', 'VBD')), 6), ((('i', 'NN'), ('feel', 'VBP')), 6), ((('life', 'NN'), ('is', 'VBZ')), 6), ((('the', 'DT'), ('lad', 'NN'), ('was', 'VBD')), 6), ((('i', 'NN'), ('asked', 'VBD')), 6), ((('i', 'NN'), ('came', 'VBD')), 6), ((('i', 'NN'), ('felt', 'VBD')), 6)]\n"
     ]
    }
   ],
   "source": [
    "# store and print the most common VP-chunks\n",
    "most_common_vp_chunks = vp_chunk_counter(vp_chunked_text)\n",
    "print(most_common_vp_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9edf45a",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "*Note: the `chunk_counters` and `tokenize_words` were imported from `chunk_counters.py` and `tokenize_words.py` which are files created by Codecademy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb167ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
